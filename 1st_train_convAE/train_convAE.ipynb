{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_convAE.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"10Flq_aIbcPRfQZ6VabydeHWKuw5S6qf7","authorship_tag":"ABX9TyP3fKSdIT4nFi4tw06AcgXy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HgjsMBdOk4Br","colab_type":"text"},"source":["Imports for different Packages needed for loading the autoencoder script, loading the dataset, saving weights and metrics"]},{"cell_type":"code","metadata":{"id":"ZQzbiFGpJF8G","colab_type":"code","colab":{}},"source":["# try:\n","#   # %tensorflow_version only exists in Colab.\n","#   %tensorflow_version 2.x\n","# except Exception:\n","#   pass\n","import tensorflow as tf\n","from tensorflow.keras.callbacks import TensorBoard, CSVLogger, ReduceLROnPlateau\n","import tensorflow_datasets as tfds\n","import sys\n","import h5py\n","import datetime"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r3Zge2IDlT9R","colab_type":"text"},"source":["Mounting Google Drive to import the convAE script"]},{"cell_type":"code","metadata":{"id":"t0dhFoqiJYCR","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6DFs94TMJZ21","colab_type":"code","colab":{}},"source":["sys.path.append('/content/drive/My Drive/convAE_2_classifier')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_zTI5o4mJeqC","colab_type":"code","colab":{}},"source":["from convAE import convAE"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lgsqLXAGliKs","colab_type":"text"},"source":["Loading the whole Dataset to train the Autoencoder on. No Validation set needed for Autoencoder"]},{"cell_type":"code","metadata":{"id":"GlYj6rzUMAmO","colab_type":"code","colab":{}},"source":["train, val = tfds.load(\n","        'cats_vs_dogs',\n","        split=['train[:99%]', 'train[99%:]'],\n","        #with_info=True,\n","        as_supervised=True, \n","        )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0D9bv1k8OPLP","colab_type":"code","colab":{}},"source":["IMG_SIZE = 256\n","input_shape = (IMG_SIZE, IMG_SIZE, 3)\n","BATCH_SIZE = 32\n","SHUFFLE_BUFFER_SIZE = 1000"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L4CPDcQMldSv","colab_type":"text"},"source":["Normailize the the vlaues to an interval between 0 and 1. And resize the images"]},{"cell_type":"code","metadata":{"id":"4Jk5DOFKL_YA","colab_type":"code","colab":{}},"source":["def format_example(image, label):\n","    image = tf.cast(image, tf.float32)\n","    image = (image/255.0) \n","    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n","    return image, label"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Jn4XU3zcJbk","colab_type":"code","colab":{}},"source":["train = train.map(format_example)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LX8dABYwl4uJ","colab_type":"text"},"source":["Create Batches"]},{"cell_type":"code","metadata":{"id":"n9Ix6zw4bOLN","colab_type":"code","colab":{}},"source":["train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n","train_x_batches = train_batches.map(lambda a, b: a)\n","train_x_batch_AE = tf.data.Dataset.zip((train_x_batches, train_x_batches))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"30yqFjCZmDEP","colab_type":"text"},"source":["Creating the Autoencoder"]},{"cell_type":"code","metadata":{"id":"qKgt3hDZJkQA","colab_type":"code","colab":{}},"source":["#tf.keras.backend.clear_session()\n","autoencoder = convAE(input_shape=input_shape)\n","autoencoder_filters = [32, 64, 128, 256]\n","# autoencoder_filters = [64, 128, 256, 512]\n","# autoencoder_filters =  [64, 128, 256, 512, 1024]\n","\n","autoencoder.create_autoencoder_with_stacked_conv(filters=autoencoder_filters)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jo-SYEJImFci","colab_type":"text"},"source":["Plot autoencoder layers"]},{"cell_type":"code","metadata":{"id":"lvzCUSH74dvH","colab_type":"code","colab":{}},"source":["tf.keras.utils.plot_model(autoencoder.encoder,\n","                          #to_file='model.png',\n","                          show_shapes=False,\n","                          show_layer_names=False,\n","                          #rankdir='TB',\n","                          expand_nested=True,\n","                          dpi=96\n",")\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_TyCLdDemIIV","colab_type":"text"},"source":["Get summary of Autoencoder Layers"]},{"cell_type":"code","metadata":{"id":"aVMHxT6iKDY9","colab_type":"code","colab":{}},"source":["autoencoder.encoder.summary()\n","autoencoder.decoder.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a9h1B2bAmN45","colab_type":"text"},"source":["Creating a file name related to the Filter sizes"]},{"cell_type":"code","metadata":{"id":"odd9QD5g7ZxK","colab_type":"code","colab":{}},"source":["listToStr = '-'.join(map(str, autoencoder_filters))\n","model_name = \"convAE_stacked\"\n","file_name = model_name + str(IMG_SIZE)+ '_' + listToStr + '_'\n","print(file_name"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E-Kvk-Y1mS1I","colab_type":"text"},"source":["Different callbacks for saving weights, logging loss in a CSV file, reducing learning rate on plateau and earlystop training if loss doesnt get better"]},{"cell_type":"code","metadata":{"id":"zyc7u389KIcd","colab_type":"code","colab":{}},"source":["dateTime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","log_dir=\"./drive/My Drive/convAE_2_classifier/logs/\" + file_name + \"_logs_\" + dateTime\n","weights_path=\"./drive/My Drive/convAE_2_classifier/weights/\" + file_name + \"_\"  + dateTime + \"_Epoch{epoch:04d}-Loss{loss:.4f}_.hdf5\"\n","csv_path = \"./drive/My Drive/convAE_2_classifier/logs/\" + file_name +  \"_model_history_\" + dateTime + \".csv\"\n","\n","save_weights_callback = tf.keras.callbacks.ModelCheckpoint(filepath=weights_path , monitor='loss', save_weights_only=True, save_freq='epoch', period=5)\n","tensorboard_callback  = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='epoch', histogram_freq=1, write_graph=True, write_images=True)\n","csv_logger = CSVLogger(csv_path, separator=',', append=True)\n","reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=4, min_lr=0.0001, mode='min',  min_delta=0.0001, cooldown=1)\n","early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0001, patience=6, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n","\n","callbacks = [save_weights_callback, tensorboard_callback, csv_logger, reduce_lr, early_stop_callback]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hvzj1RaEmXFi","colab_type":"text"},"source":["Function to get learningrate metric while training"]},{"cell_type":"code","metadata":{"id":"i9BZWGf1K5GW","colab_type":"code","colab":{}},"source":["def get_lr_metric(optimizer):\n","    def lr(y_true, y_pred):\n","      return optimizer.lr \n","    return lr"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sXyRS5SfmfPT","colab_type":"text"},"source":["Compiling the Model"]},{"cell_type":"code","metadata":{"id":"v9pbEM9FK-9A","colab_type":"code","colab":{}},"source":["optimizer = tf.keras.optimizers.SGD(learning_rate=0.02, momentum=0.9, nesterov=False)\n","lossfn = tf.keras.losses.MeanSquaredError()\n","lr_metric = get_lr_metric(optimizer)\n","\n","autoencoder.autoencoder.compile(optimizer=optimizer, loss=lossfn, metrics=[ lr_metric])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XU8r_oWLmh0F","colab_type":"text"},"source":["Train the Model"]},{"cell_type":"code","metadata":{"id":"bRktMn6ILUxF","colab_type":"code","colab":{}},"source":["history = autoencoder.autoencoder.fit(train_x_batch_AE,\n","                                       epochs=50,\n","                                       #initial_epoch=history.epoch[-1],\n","                                       shuffle=True,\n","                                       callbacks=callbacks)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vjqY6CLjmpOQ","colab_type":"text"},"source":["Save the last Weights of the Model"]},{"cell_type":"code","metadata":{"id":"fQiNt9D5hrqo","colab_type":"code","colab":{}},"source":["last_loss = history.history['loss'][-1]\n","epoch = len(history.history['loss'])\n","combined = F\"_Epoch{epoch:04d}-Loss{last_loss:.6f}\"\n","autoencoder.autoencoder.save_weights(\"./drive/My Drive/convAE_2_classifier/weights/\" + file_name + \"_\" + dateTime + \"_\" + combined +  \"best.hdf5\")"],"execution_count":0,"outputs":[]}]}